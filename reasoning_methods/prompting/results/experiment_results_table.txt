# Llama 3.2 Reasoning Experiment Results

## Results organized by dataset, model size, and prompting method

+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| Dataset        | Model                     | Simple   | Simple+SC   | CoT   | CoT+SC   | Role   | Role+SC   | Plan   | Plan+SC   |
+================+===========================+==========+=============+=======+==========+========+===========+========+===========+
| gsm8k          | 1b                        | 5.5%     | 5.8%        | 4.9%  | 8.8%     | 2.8%   | 5.4%      | 3.2%   | 3.8%      |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 1b-instruct               | 27.1%    | 42.1%       | 34.4% | 51.8%    | 32.3%  | 50.3%     | 28.2%  | 51.1%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 1b-sft-full               | 5.0%     | 6.9%        | 5.5%  | 9.7%     | 4.8%   | 5.4%      | 4.2%   | 4.6%      |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 1b-sft-full-slimorca-100k | 3.8%     | 6.9%        | 7.5%  | 10.0%    | 3.6%   | -         | 5.0%   | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 1b-sft-lora               | 4.5%     | -           | 6.4%  | -        | 3.1%   | -         | 5.1%   | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 1b-sft-lora-all           | 3.9%     | -           | 5.3%  | -        | 3.8%   | -         | 4.0%   | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 1b-sft-mixed-best         | 6.9%     | 18.4%       | 15.3% | 26.1%    | 10.8%  | 24.7%     | 14.1%  | 29.1%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | 3b                        | 12.2%    | -           | 14.3% | 29.5%    | 7.0%   | -         | 10.5%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| gsm8k          | star                      | 8.2%     | -           | 5.0%  | -        | 4.7%   | -         | 5.2%   | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| arc            | 1b                        | 39.8%    | 37.8%       | 36.5% | 34.8%    | 31.3%  | 30.8%     | 36.4%  | 36.6%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| arc            | 1b-instruct               | 56.1%    | -           | 55.1% | -        | 46.8%  | -         | 55.2%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| arc            | 1b-sft-full-slimorca-100k | 39.7%    | -           | 36.3% | -        | 31.3%  | -         | 35.7%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| arc            | 1b-sft-mixed-best         | 45.6%    | -           | 41.2% | -        | 39.2%  | -         | 39.5%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| arc            | 3b                        | 62.9%    | -           | 54.1% | -        | 52.0%  | -         | 54.2%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| arc            | star                      | 35.1%    | -           | 33.0% | -        | 29.2%  | -         | 32.6%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | 1b                        | 44.0%    | 42.4%       | 37.6% | 38.3%    | 35.4%  | 35.1%     | 36.1%  | 37.0%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | 1b-sft-full               | 44.8%    | -           | 38.0% | -        | 35.0%  | -         | 36.5%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | 1b-sft-full-slimorca-100k | 44.6%    | -           | 37.8% | -        | 35.4%  | -         | 36.5%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | 1b-sft-lora-all           | 44.8%    | -           | 37.7% | -        | 36.3%  | -         | 36.5%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | 1b-sft-mixed-best         | 53.9%    | -           | 43.5% | -        | 44.0%  | -         | 44.2%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | 3b                        | 68.3%    | -           | 55.4% | -        | 54.3%  | -         | 57.8%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| race           | star                      | 35.0%    | -           | 38.0% | -        | 33.9%  | -         | 35.4%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| mmlu           | 1b                        | 41.0%    | 38.3%       | 37.2% | 34.8%    | 33.2%  | 31.7%     | 36.4%  | 36.3%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| mmlu           | 1b-sft-mixed-best         | 40.2%    | -           | 37.7% | -        | 35.9%  | -         | 37.7%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| mmlu           | 3b                        | 55.7%    | -           | 45.8% | -        | 46.2%  | -         | 46.4%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| mmlu           | star                      | 31.8%    | -           | 32.8% | -        | 31.4%  | -         | 33.6%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| drop           | 1b                        | 6.7%     | 9.5%        | 4.4%  | 9.3%     | 4.9%   | 8.4%      | 3.4%   | 6.3%      |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| drop           | 1b-instruct               | 50.0%    | 47.9%       | 40.3% | 45.4%    | 54.7%  | 53.9%     | 43.7%  | 46.1%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| drop           | 3b                        | 47.0%    | -           | 10.2% | -        | 6.1%   | -         | 11.2%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| drop           | star                      | 9.2%     | -           | 5.9%  | -        | 10.9%  | -         | 5.5%   | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| commonsense_qa | 1b                        | 42.1%    | -           | 35.7% | -        | 27.1%  | -         | 37.7%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| commonsense_qa | 1b-instruct               | 48.9%    | 49.0%       | 53.7% | -        | 45.4%  | -         | 56.1%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| commonsense_qa | 1b-sft-mixed-best         | 42.6%    | 43.6%       | 55.9% | 54.1%    | 55.6%  | 55.5%     | 58.3%  | 58.7%     |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| commonsense_qa | 3b                        | 54.4%    | -           | 53.9% | -        | 52.2%  | -         | 53.6%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+
| commonsense_qa | star                      | 42.5%    | -           | 40.6% | -        | 34.0%  | -         | 41.1%  | -         |
+----------------+---------------------------+----------+-------------+-------+----------+--------+-----------+--------+-----------+


## Best Performance by Dataset

+----------------+-----------------------------+------------+
| Dataset        | Best Method                 | Accuracy   |
+================+=============================+============+
| GSM8K          | Cot+SC (1B-INSTRUCT)        | 51.8%      |
+----------------+-----------------------------+------------+
| ARC            | Simple (3B)                 | 62.9%      |
+----------------+-----------------------------+------------+
| RACE           | Simple (3B)                 | 68.3%      |
+----------------+-----------------------------+------------+
| MMLU           | Simple (3B)                 | 55.7%      |
+----------------+-----------------------------+------------+
| DROP           | Role (1B-INSTRUCT)          | 54.7%      |
+----------------+-----------------------------+------------+
| COMMONSENSE_QA | Plan+SC (1B-SFT-MIXED-BEST) | 58.7%      |
+----------------+-----------------------------+------------+


## Model Size Impact (3B vs 1B)



Average improvement across all templates and datasets: 0.0 percentage points

## Self-Consistency Effect (1B model)

+-----------+------------+---------+-----------+-------------+
| Dataset   | Template   | No SC   | With SC   | SC Effect   |
+===========+============+=========+===========+=============+
| gsm8k     | Cot        | 4.9%    | 8.8%      | 3.9pp       |
+-----------+------------+---------+-----------+-------------+
| gsm8k     | Plan       | 3.2%    | 3.8%      | 0.6pp       |
+-----------+------------+---------+-----------+-------------+
| gsm8k     | Role       | 2.8%    | 5.4%      | 2.6pp       |
+-----------+------------+---------+-----------+-------------+
| gsm8k     | Simple     | 5.5%    | 5.8%      | 0.3pp       |
+-----------+------------+---------+-----------+-------------+
| arc       | Cot        | 36.5%   | 34.8%     | -1.7pp      |
+-----------+------------+---------+-----------+-------------+
| arc       | Plan       | 36.4%   | 36.6%     | 0.2pp       |
+-----------+------------+---------+-----------+-------------+
| arc       | Role       | 31.3%   | 30.8%     | -0.5pp      |
+-----------+------------+---------+-----------+-------------+
| arc       | Simple     | 39.8%   | 37.8%     | -2.0pp      |
+-----------+------------+---------+-----------+-------------+
| race      | Cot        | 37.6%   | 38.3%     | 0.7pp       |
+-----------+------------+---------+-----------+-------------+
| race      | Plan       | 36.1%   | 37.0%     | 0.9pp       |
+-----------+------------+---------+-----------+-------------+
| race      | Role       | 35.4%   | 35.1%     | -0.3pp      |
+-----------+------------+---------+-----------+-------------+
| race      | Simple     | 44.0%   | 42.4%     | -1.6pp      |
+-----------+------------+---------+-----------+-------------+
| mmlu      | Cot        | 37.2%   | 34.8%     | -2.4pp      |
+-----------+------------+---------+-----------+-------------+
| mmlu      | Plan       | 36.4%   | 36.3%     | -0.1pp      |
+-----------+------------+---------+-----------+-------------+
| mmlu      | Role       | 33.2%   | 31.7%     | -1.5pp      |
+-----------+------------+---------+-----------+-------------+
| mmlu      | Simple     | 41.0%   | 38.3%     | -2.7pp      |
+-----------+------------+---------+-----------+-------------+
| drop      | Cot        | 4.4%    | 9.3%      | 4.9pp       |
+-----------+------------+---------+-----------+-------------+
| drop      | Plan       | 3.4%    | 6.3%      | 2.9pp       |
+-----------+------------+---------+-----------+-------------+
| drop      | Role       | 4.9%    | 8.4%      | 3.5pp       |
+-----------+------------+---------+-----------+-------------+
| drop      | Simple     | 6.7%    | 9.5%      | 2.8pp       |
+-----------+------------+---------+-----------+-------------+

Average self-consistency effect: 0.5 percentage points

## Prompt Template Effect (vs Simple Template)

+---------------------------+---------+----------+---------+-----------+---------+-----------+
|                           | Cot     | Cot+SC   | Plan    | Plan+SC   | Role    | Role+SC   |
+===========================+=========+==========+=========+===========+=========+===========+
| 1b                        | -3.8pp  | -1.6pp   | -4.3pp  | -2.8pp    | -7.4pp  | -4.5pp    |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 1b-instruct               | 0.3pp   | 3.6pp    | 0.3pp   | 3.6pp     | -0.7pp  | 7.1pp     |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 1b-sft-full               | -3.1pp  | 2.8pp    | -4.5pp  | -2.3pp    | -5.0pp  | -1.5pp    |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 1b-sft-full-slimorca-100k | -2.2pp  | 3.1pp    | -3.6pp  | -         | -5.9pp  | -         |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 1b-sft-lora               | 1.9pp   | -        | 0.6pp   | -         | -1.4pp  | -         |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 1b-sft-lora-all           | -2.8pp  | -        | -4.1pp  | -         | -4.3pp  | -         |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 1b-sft-mixed-best         | 0.9pp   | 9.1pp    | 0.9pp   | 12.9pp    | -0.7pp  | 9.1pp     |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| 3b                        | -11.1pp | -        | -11.1pp | -         | -13.8pp | -         |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| star                      | -1.2pp  | -        | -1.4pp  | -         | -2.9pp  | -         |
+---------------------------+---------+----------+---------+-----------+---------+-----------+
| Average                   | -2.4pp  | 3.4pp    | -3.0pp  | 2.9pp     | -4.7pp  | 2.6pp     |
+---------------------------+---------+----------+---------+-----------+---------+-----------+


## Finetuning Effect (vs 1B Base Model)

+---------------------------+---------------------------+
| Finetuned Model           | Average Gain vs 1B Base   |
+===========================+===========================+
| 1b-instruct               | 30.7pp                    |
+---------------------------+---------------------------+
| 1b-sft-full               | 0.6pp                     |
+---------------------------+---------------------------+
| 1b-sft-full-slimorca-100k | 0.4pp                     |
+---------------------------+---------------------------+
| 1b-sft-lora               | 0.7pp                     |
+---------------------------+---------------------------+
| 1b-sft-lora-all           | 0.3pp                     |
+---------------------------+---------------------------+
| 1b-sft-mixed-best         | 9.7pp                     |
+---------------------------+---------------------------+
| star                      | -0.1pp                    |
+---------------------------+---------------------------+

